{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jealousy toy boy friendship friends Tom Hanks ...\n",
       "1    board game disappearance based on children's b...\n",
       "2    fishing best friend duringcreditsstinger old m...\n",
       "3    based on novel interracial relationship single...\n",
       "Name: soup, dtype: object"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"soup\"].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading nltk stopwords.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " os.chdir(r\"C:\\Users\\HP\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=pandas.read_csv('movies_metadata.csv',low_memory=False)\n",
    "keywords=pandas.read_csv('keywords.csv')\n",
    "credits=pandas.read_csv('credits.csv')\n",
    "metadata=metadata.drop([19730,29503,35587])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords['id']=keywords['id'].astype('int')\n",
    "metadata['id']=metadata['id'].astype('int')\n",
    "credits['id']=credits['id'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=metadata.merge(keywords,on='id')\n",
    "metadata=metadata.merge(credits,on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(data):\n",
    "    \n",
    "    for i in data:\n",
    "        if (i['job']=='Director'):\n",
    "            return i[\"name\"]\n",
    "        \n",
    "        return np.NaN\n",
    "    \n",
    "    \n",
    "def get_director(data):\n",
    "    \n",
    "    for i in data:\n",
    "        if (i['job']=='Director'):\n",
    "            return i[\"name\"]\n",
    "        \n",
    "        return np.NaN\n",
    "    \n",
    "def stemmer(data):\n",
    "    #assumption takes in a tokenized form of data\n",
    "    stemmer=SnowballStemmer(\"english\")\n",
    "    text=[]\n",
    "    for i in data:\n",
    "        \n",
    "        text.append([stemmer.stem(word) for word in i])\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def get_list(data):\n",
    "    \n",
    "    names=[i['name'] for i in data]\n",
    "     \n",
    "    \n",
    "    if(len(names)>5):\n",
    "        names=names[:5]\n",
    "    \n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(x):\n",
    "    \n",
    "    if isinstance(x,list):\n",
    "        \n",
    "        names=[str.lower(i.replace(\" \",\"\")) for i in data]\n",
    "    if isinstance(x,str):\n",
    "        x=str.lower(x.replace(\" \",\"\"))\n",
    "    \n",
    "    \n",
    "def create_soup(data):\n",
    "    return \" \".join(data[\"keywords\"])+\" \"+\" \".join(data[\"cast\"])+\" \"+data[\"director\"]+\" \"+\" \".join(data[\"genres\"])\n",
    "\n",
    "def create_soup2(data):\n",
    "    return data[\"soup\"]+\" \"+data[\"title\"]\n",
    "\n",
    "def create_soup3(data):\n",
    "    \n",
    "    data[\"tagline\"]=data[\"tagline\"].apply(remove_white)\n",
    "    return data[\"soup\"]+\" \"+data[\"tagline\"]\n",
    "\n",
    "def clean_data(x):\n",
    "    names=[str(i.replace(\" \",\"\")) for i in data]\n",
    "    return names;\n",
    "\n",
    "def remove_white(data):\n",
    "    if isinstance(data,str):\n",
    "        data=str.lower(data.replace(\" \",\"\"))\n",
    "        return data\n",
    "    \n",
    "    return np.NaN\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    #creating a list of stop-words\n",
    "    stoplist=set(stopwords.words('english'))\n",
    "    text=[]\n",
    "    #lower case each document ,split it by white space and filter out stopwords.\n",
    "    \n",
    "    for i in data:\n",
    "        \n",
    "        text.append([word.lower() for word in i if word not in stoplist ])\n",
    "    return text\n",
    "\n",
    "def tokenize(data):\n",
    "    temp=list(data)\n",
    "    temp=[str.lower(i) for i in temp]\n",
    "    temp=[list(i.split(' ')) for i in temp]\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def model(data):\n",
    "    \n",
    "    model_=Word2Vec(data,window=3,min_count=1,sg=1,workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    return model_\n",
    "\n",
    "def model2(data):\n",
    "    \n",
    "    model_=Word2Vec(data,window=3,min_count=1,sg=1,workers=multiprocessing.cpu_count(),compute_loss=True,negative=15)\n",
    "    \n",
    "    return model_\n",
    "\n",
    "\n",
    "def cleaner(data):\n",
    "    data=[str.lower(i) for i in data.splitlines()]\n",
    "    \n",
    "    \n",
    "    \n",
    "def find10_most_similar(model,text):\n",
    "    text=text.lower()\n",
    "    list_of_similar=model.wv.most_similar(text)\n",
    "    return list_of_similar\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing nulls from the data\n",
    "metadata.fillna(\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['cast','keywords','crew','genres']\n",
    "\n",
    "for i in features:\n",
    "    metadata[i]=metadata[i].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['director']=metadata['crew'].apply(get_director)\n",
    "metadata['director'].fillna(\" \",inplace=True)\n",
    "metadata['director']=metadata['director'].apply(remove_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    metadata[i]=metadata[i].apply(get_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['title']=metadata['title'].apply(remove_white )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"soup\"]=metadata.apply(create_soup,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"soup\"]=metadata.apply(create_soup2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=tokenize(metadata[\"soup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=remove_stop_words(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram=model(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    " #finding similarity between all our titles and the query word\n",
    "def find_similar(query):\n",
    "    similar=[]\n",
    "    \n",
    "    #find the similarity between the query word and all of our titles in the collection.\n",
    "    for i in metadata['title']:\n",
    "        try:\n",
    "            similarity=skipgram.wv.similarity(query,i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    " \n",
    "        #print(\"similarity between {} and {} is {}\".format(query,i,similarity))\n",
    "        if i == query:\n",
    "            continue\n",
    "            \n",
    "        similar.append((i,similarity))\n",
    "        \n",
    "            \n",
    "        #we should get all scores and find the score that lies in the 90th percentile of all the scores\n",
    "    similar=list(set(similar))\n",
    "    similar=pd.DataFrame(similar,columns=['title','score'])\n",
    "    \n",
    "    #required to sort the dataframe in the order of priority.\n",
    "    \n",
    "            \n",
    "    return similar.sort_values(by='score',ascending=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following class will model the user session object\n",
    "class user(object):\n",
    "    def __init__(self):\n",
    "          self.latest_search='jumanji'\n",
    "            \n",
    "    def similar(self):\n",
    "        return find_similar(self.latest_search)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the user object:\n",
    "user_1=user()p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>thelife&amp;adventuresofsantaclaus</td>\n",
       "      <td>0.942630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>bridgetoterabithia</td>\n",
       "      <td>0.936231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25358</th>\n",
       "      <td>pokémon:spelloftheunknown</td>\n",
       "      <td>0.932714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37832</th>\n",
       "      <td>journeytothecenteroftheearth</td>\n",
       "      <td>0.930619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12781</th>\n",
       "      <td>alittleprincess</td>\n",
       "      <td>0.929734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>rudolphthered-nosedreindeer</td>\n",
       "      <td>0.898291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30800</th>\n",
       "      <td>mostlyghostly</td>\n",
       "      <td>0.898188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>saintseiyaheavenchapter:overture</td>\n",
       "      <td>0.898070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>garfield:atailoftwokitties</td>\n",
       "      <td>0.897511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>curiousgeorge:averymonkeychristmas</td>\n",
       "      <td>0.897204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title     score\n",
       "444        thelife&adventuresofsantaclaus  0.942630\n",
       "21522                  bridgetoterabithia  0.936231\n",
       "25358           pokémon:spelloftheunknown  0.932714\n",
       "37832        journeytothecenteroftheearth  0.930619\n",
       "12781                     alittleprincess  0.929734\n",
       "...                                   ...       ...\n",
       "24124         rudolphthered-nosedreindeer  0.898291\n",
       "30800                       mostlyghostly  0.898188\n",
       "11428    saintseiyaheavenchapter:overture  0.898070\n",
       "391            garfield:atailoftwokitties  0.897511\n",
       "22221  curiousgeorge:averymonkeychristmas  0.897204\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_1.similar()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#module to generate skipgrams\n",
    "from keras.preprocessing.sequence import skipgrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114364\n",
      "Vocabulary Sample: [('drama', 1), ('comedy', 2), ('thriller', 3), ('romance', 4), ('action', 5), ('horror', 6), ('crime', 7), ('documentary', 8), ('adventure', 9), ('family', 10)]\n"
     ]
    }
   ],
   "source": [
    "#building the vocabulary for our textual data\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(metadata['soup'])\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in metadata['soup']]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(friends (576), there'snoplacelikeutopia (40164)) -> 0\n",
      "(allen (126), don (151)) -> 1\n",
      "(boy (541), jim (99)) -> 1\n",
      "(boy (541), hasaj (67501)) -> 0\n",
      "(animation (21), allen (126)) -> 1\n",
      "(jim (99), biplane (29881)) -> 0\n",
      "(varney (5531), sale (13861)) -> 0\n",
      "(animation (21), mirchoff (14607)) -> 0\n",
      "(jim (99), aportraitofcomposermortenlauridsen (70323)) -> 0\n",
      "(wallace (640), cagedheat3000 (114350)) -> 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer concatenate_2 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x0000020308FFFE08>, <keras.engine.sequential.Sequential object at 0x00000203E655ED08>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m--> 697\u001b[1;33m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.engine.sequential.Sequential'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-117e0169d24a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"glorot_uniform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mean_squared_error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rmsprop\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    314\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full input: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. All inputs to the layer '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                                  'should be tensors.')\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer concatenate_2 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x0000020308FFFE08>, <keras.engine.sequential.Sequential object at 0x00000203E655ED08>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "word_model.add(Embedding(vocab_size, embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))\n",
    "\n",
    "word_model.add(Reshape((embed_size, )))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embed_size,\n",
    "                  embeddings_initializer=\"glorot_uniform\",\n",
    "                  input_length=1))\n",
    "context_model.add(Reshape((embed_size,)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Concatenate()([word_model, context_model]))\n",
    "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#enter filepath to the model\n",
    "path=''\n",
    "data=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
